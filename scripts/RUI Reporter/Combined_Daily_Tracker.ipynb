{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7870ec",
   "metadata": {},
   "source": [
    "# RUI Registration Daily Tracker - HuBMAP & SenNet Combined Analysis\n",
    "\n",
    "This notebook tracks RUI location registration coverage for both HuBMAP and SenNet datasets over time. It generates daily counts for:\n",
    "- Total datasets\n",
    "- Supported datasets (in organs covered by reference anatomy)\n",
    "- Registered datasets (with RUI locations)\n",
    "\n",
    "Results are saved to a CSV file with dates as columns and metrics as rows for time-series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160637f0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6575f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Current date/time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a188b6",
   "metadata": {},
   "source": [
    "## 2. Set API Tokens and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2214dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Tokens (Replace with your actual tokens)\n",
    "HUBMAP_TOKEN = \"xyz\"\n",
    "SENNET_TOKEN = \"xyz\"  # Replace with actual SenNet token\n",
    "\n",
    "# API Endpoints\n",
    "HUBMAP_SEARCH_API_URL = \"https://search.api.hubmapconsortium.org/v3/search\"\n",
    "SENNET_SEARCH_API_URL = \"https://search.api.sennetconsortium.org/v3/search\"\n",
    "REFERENCE_ORGANS_URL = \"https://apps.humanatlas.io/api/v1/reference-organs\"\n",
    "\n",
    "# Headers for API calls\n",
    "hubmap_headers = {\"Authorization\": f\"Bearer {HUBMAP_TOKEN}\"}\n",
    "sennet_headers = {\"Authorization\": f\"Bearer {SENNET_TOKEN}\"}\n",
    "\n",
    "# Output CSV file path\n",
    "CSV_OUTPUT_PATH = \"/Users/dequeue/Desktop/RUI.nosync/hra-registrations/scripts/combined_daily_counts.csv\"\n",
    "\n",
    "print(\"API configuration completed\")\n",
    "print(f\"HuBMAP API: {HUBMAP_SEARCH_API_URL}\")\n",
    "print(f\"SenNet API: {SENNET_SEARCH_API_URL}\")\n",
    "print(f\"Output CSV: {CSV_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47cdbda",
   "metadata": {},
   "source": [
    "## 3. Extract code_to_uberon Mapping from metadata.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to metadata.js file\n",
    "metadata_path = \"/Users/dequeue/Desktop/RUI.nosync/hra-registrations/scripts/RUI Reporter/metadata.js\"\n",
    "\n",
    "# Extract code_to_uberon mapping from metadata.js\n",
    "code_to_uberon = {}\n",
    "\n",
    "try:\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "        # Parse the organ mappings using regex\n",
    "        matches = re.findall(r\"(\\w+):\\s*{\\s*code: '(\\w+)',\\s*label: '[^']+',\\s*organ_id: '([^']+)'\", content)\n",
    "        for code, _, uberon in matches:\n",
    "            code_to_uberon[code] = uberon\n",
    "    \n",
    "    print(f\"Successfully extracted {len(code_to_uberon)} organ code mappings\")\n",
    "    print(\"Sample mappings:\", dict(list(code_to_uberon.items())[:5]))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: metadata.js file not found at {metadata_path}\")\n",
    "    print(\"Using empty mapping - this may result in no supported datasets\")\n",
    "    code_to_uberon = {}\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing metadata.js: {e}\")\n",
    "    code_to_uberon = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51080b84",
   "metadata": {},
   "source": [
    "## 4. Fetch Reference Organs and Normalize UBERON IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize UBERON IDs from IRI format to CURIE format\n",
    "def iri_to_curie(iri):\n",
    "    \"\"\"Convert UBERON IRI to CURIE format (e.g., UBERON:0001234)\"\"\"\n",
    "    if iri and \"obo/UBERON_\" in iri:\n",
    "        return \"UBERON:\" + iri.split(\"_\")[-1]\n",
    "    return iri\n",
    "\n",
    "# Fetch all reference organs from HRA API\n",
    "try:\n",
    "    response = requests.get(REFERENCE_ORGANS_URL)\n",
    "    response.raise_for_status()\n",
    "    organs = response.json()\n",
    "    \n",
    "    # Extract and normalize UBERON IDs\n",
    "    reference_uberon_ids = {\n",
    "        iri_to_curie(organ.get(\"representation_of\")) \n",
    "        for organ in organs \n",
    "        if organ.get(\"representation_of\")\n",
    "    }\n",
    "    \n",
    "    print(f\"Successfully fetched {len(organs)} reference organs\")\n",
    "    print(f\"Extracted {len(reference_uberon_ids)} unique UBERON IDs\")\n",
    "    print(\"Sample UBERON IDs:\", list(reference_uberon_ids)[:5])\n",
    "    \n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error fetching reference organs: {e}\")\n",
    "    reference_uberon_ids = set()\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    reference_uberon_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9d71e",
   "metadata": {},
   "source": [
    "## 5. Filter Supported Codes for HuBMAP and SenNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter organ codes to only those whose UBERON IDs are in the reference set\n",
    "filtered_supported_codes = [\n",
    "    code for code, uberon in code_to_uberon.items() \n",
    "    if uberon in reference_uberon_ids\n",
    "]\n",
    "\n",
    "print(f\"Total organ codes in metadata: {len(code_to_uberon)}\")\n",
    "print(f\"Supported organ codes (in reference anatomy): {len(filtered_supported_codes)}\")\n",
    "print(f\"Supported codes: {filtered_supported_codes}\")\n",
    "\n",
    "# This list will be used for both HuBMAP and SenNet queries\n",
    "# as both consortia use the same organ code system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2129ef0",
   "metadata": {},
   "source": [
    "## 6. Query and Count Datasets for HuBMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hubmap_datasets():\n",
    "    \"\"\"Query HuBMAP API for dataset counts\"\"\"\n",
    "    \n",
    "    # 1. Total count of all datasets\n",
    "    total_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    # 2. Count of datasets where organ matches supported codes\n",
    "    supported_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                    {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                    {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    # 3. Count of datasets with organ in supported codes AND rui_location present\n",
    "    registered_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                    {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                    {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}},\n",
    "                    {\"exists\": {\"field\": \"ancestors.rui_location\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute queries\n",
    "        total_resp = requests.post(HUBMAP_SEARCH_API_URL, json=total_query, headers=hubmap_headers)\n",
    "        supported_resp = requests.post(HUBMAP_SEARCH_API_URL, json=supported_query, headers=hubmap_headers)\n",
    "        registered_resp = requests.post(HUBMAP_SEARCH_API_URL, json=registered_query, headers=hubmap_headers)\n",
    "        \n",
    "        # Extract counts\n",
    "        total_count = total_resp.json()['hits']['total']['value']\n",
    "        supported_count = supported_resp.json()['hits']['total']['value']\n",
    "        registered_count = registered_resp.json()['hits']['total']['value']\n",
    "        \n",
    "        print(\"HuBMAP Dataset Counts:\")\n",
    "        print(f\"  Total datasets: {total_count}\")\n",
    "        print(f\"  Supported datasets: {supported_count}\")\n",
    "        print(f\"  Registered datasets: {registered_count}\")\n",
    "        \n",
    "        return total_count, supported_count, registered_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying HuBMAP API: {e}\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "# Execute HuBMAP queries\n",
    "hubmap_total, hubmap_supported, hubmap_registered = query_hubmap_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d40c3c",
   "metadata": {},
   "source": [
    "## 7. Query and Count Datasets for SenNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb487ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sennet_datasets():\n",
    "    \"\"\"Query SenNet API for dataset counts\"\"\"\n",
    "    \n",
    "    # Same query structure as HuBMAP but using SenNet endpoint\n",
    "    # 1. Total count of all datasets\n",
    "    total_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    # 2. Count of datasets where organ matches supported codes  \n",
    "    supported_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                    {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                    {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    # 3. Count of datasets with organ in supported codes AND rui_location present\n",
    "    registered_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                    {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                    {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}},\n",
    "                    {\"exists\": {\"field\": \"ancestors.rui_location\"}}\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute queries\n",
    "        total_resp = requests.post(SENNET_SEARCH_API_URL, json=total_query, headers=sennet_headers)\n",
    "        supported_resp = requests.post(SENNET_SEARCH_API_URL, json=supported_query, headers=sennet_headers)\n",
    "        registered_resp = requests.post(SENNET_SEARCH_API_URL, json=registered_query, headers=sennet_headers)\n",
    "        \n",
    "        # Extract counts\n",
    "        total_count = total_resp.json()['hits']['total']['value']\n",
    "        supported_count = supported_resp.json()['hits']['total']['value']\n",
    "        registered_count = registered_resp.json()['hits']['total']['value']\n",
    "        \n",
    "        print(\"SenNet Dataset Counts:\")\n",
    "        print(f\"  Total datasets: {total_count}\")\n",
    "        print(f\"  Supported datasets: {supported_count}\")\n",
    "        print(f\"  Registered datasets: {registered_count}\")\n",
    "        \n",
    "        return total_count, supported_count, registered_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying SenNet API: {e}\")\n",
    "        print(\"Note: Make sure you have a valid SenNet API token\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "# Execute SenNet queries\n",
    "sennet_total, sennet_supported, sennet_registered = query_sennet_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16210b1",
   "metadata": {},
   "source": [
    "## 8. Write Counts to CSV with Date as Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_counts_to_csv():\n",
    "    \"\"\"Write the collected counts to CSV with dates as columns\"\"\"\n",
    "    \n",
    "    # Get current timestamp for column header\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Prepare data: each row is a metric, each column is a date\n",
    "    new_data = {\n",
    "        'HuBMAP Total Datasets': hubmap_total,\n",
    "        'HuBMAP Supported Datasets': hubmap_supported, \n",
    "        'HuBMAP Registered Datasets': hubmap_registered,\n",
    "        'SenNet Total Datasets': sennet_total,\n",
    "        'SenNet Supported Datasets': sennet_supported,\n",
    "        'SenNet Registered Datasets': sennet_registered\n",
    "    }\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    file_exists = os.path.isfile(CSV_OUTPUT_PATH)\n",
    "    \n",
    "    if not file_exists:\n",
    "        # Create new CSV file with headers and first data column\n",
    "        with open(CSV_OUTPUT_PATH, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            # Write header row with metric names and first date\n",
    "            header = ['Metric'] + [current_date]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Write data rows\n",
    "            for metric, value in new_data.items():\n",
    "                writer.writerow([metric, value])\n",
    "        \n",
    "        print(f\"Created new CSV file: {CSV_OUTPUT_PATH}\")\n",
    "        \n",
    "    else:\n",
    "        # Read existing CSV and add new column\n",
    "        with open(CSV_OUTPUT_PATH, 'r', newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        if len(rows) == 0:\n",
    "            # Empty file, treat as new\n",
    "            rows = [['Metric'], ['HuBMAP Total Datasets'], ['HuBMAP Supported Datasets'], \n",
    "                   ['HuBMAP Registered Datasets'], ['SenNet Total Datasets'], \n",
    "                   ['SenNet Supported Datasets'], ['SenNet Registered Datasets']]\n",
    "        \n",
    "        # Add new date to header\n",
    "        rows[0].append(current_date)\n",
    "        \n",
    "        # Add new data to each metric row\n",
    "        metric_to_row = {}\n",
    "        for i, row in enumerate(rows[1:], 1):\n",
    "            if len(row) > 0:\n",
    "                metric_to_row[row[0]] = i\n",
    "        \n",
    "        # Ensure all metrics exist and add new values\n",
    "        for metric, value in new_data.items():\n",
    "            if metric in metric_to_row:\n",
    "                rows[metric_to_row[metric]].append(value)\n",
    "            else:\n",
    "                # Add new metric row\n",
    "                new_row = [metric] + [''] * (len(rows[0]) - 2) + [value]\n",
    "                rows.append(new_row)\n",
    "        \n",
    "        # Pad any short rows\n",
    "        max_cols = len(rows[0])\n",
    "        for row in rows:\n",
    "            while len(row) < max_cols:\n",
    "                row.append('')\n",
    "        \n",
    "        # Write updated CSV\n",
    "        with open(CSV_OUTPUT_PATH, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(f\"Updated existing CSV file: {CSV_OUTPUT_PATH}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\\\nData added for {current_date}:\")\n",
    "    for metric, value in new_data.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Execute CSV writing\n",
    "write_counts_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624940e",
   "metadata": {},
   "source": [
    "## 9. Schedule Daily Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b2de1",
   "metadata": {},
   "source": [
    "### Option 1: Using cron (macOS/Linux)\n",
    "\n",
    "To schedule this notebook to run daily, you can:\n",
    "\n",
    "1. **Convert to Python script**: Export this notebook as a `.py` file\n",
    "2. **Add to crontab**: \n",
    "   ```bash\n",
    "   # Edit crontab\n",
    "   crontab -e\n",
    "   \n",
    "   # Add this line to run daily at 9 AM\n",
    "   0 9 * * * /usr/bin/python3 /Users/dequeue/Desktop/RUI.nosync/hra-registrations/scripts/RUI\\ Reporter/combined_daily_tracker.py\n",
    "   ```\n",
    "\n",
    "### Option 2: Using Python scheduler (within this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Run scheduler within Python (uncomment to use)\n",
    "# Note: This requires keeping the notebook/script running continuously\n",
    "\n",
    "\"\"\"\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def daily_data_collection():\n",
    "    print(f\"Starting daily data collection at {datetime.now()}\")\n",
    "    \n",
    "    # Re-run all the data collection functions\n",
    "    hubmap_total, hubmap_supported, hubmap_registered = query_hubmap_datasets()\n",
    "    sennet_total, sennet_supported, sennet_registered = query_sennet_datasets() \n",
    "    write_counts_to_csv()\n",
    "    \n",
    "    print(\"Daily data collection completed\")\n",
    "\n",
    "# Schedule the job for 9 AM daily\n",
    "schedule.every().day.at(\"09:00\").do(daily_data_collection)\n",
    "\n",
    "# Keep the script running (uncomment next lines to activate)\n",
    "# print(\"Scheduler started. Press Ctrl+C to stop.\")\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(60)  # Check every minute\n",
    "\"\"\"\n",
    "\n",
    "print(\"Scheduling options provided above.\")\n",
    "print(\"Choose Option 1 (cron) for production use, or Option 2 for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad59bf",
   "metadata": {},
   "source": [
    "## Summary and Data Visualization\n",
    "\n",
    "View the generated CSV file to track trends over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db62ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current CSV contents\n",
    "if os.path.exists(CSV_OUTPUT_PATH):\n",
    "    print(f\"Current contents of {CSV_OUTPUT_PATH}:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Read and display as DataFrame for better formatting\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_OUTPUT_PATH, index_col=0)\n",
    "        print(df)\n",
    "        \n",
    "        print(f\"\\nCSV file structure:\")\n",
    "        print(f\"  Rows (metrics): {len(df)}\")\n",
    "        print(f\"  Columns (dates): {len(df.columns)}\")\n",
    "        print(f\"  Latest data: {df.columns[-1] if len(df.columns) > 0 else 'None'}\")\n",
    "        \n",
    "        # Calculate registration percentages for latest data\n",
    "        if len(df.columns) > 0:\n",
    "            latest_col = df.columns[-1]\n",
    "            print(f\"\\nRegistration Coverage for {latest_col}:\")\n",
    "            \n",
    "            # HuBMAP percentages\n",
    "            hubmap_supported_latest = df.loc['HuBMAP Supported Datasets', latest_col]\n",
    "            hubmap_registered_latest = df.loc['HuBMAP Registered Datasets', latest_col]\n",
    "            if hubmap_supported_latest > 0:\n",
    "                hubmap_coverage = (hubmap_registered_latest / hubmap_supported_latest) * 100\n",
    "                print(f\"  HuBMAP: {hubmap_registered_latest}/{hubmap_supported_latest} = {hubmap_coverage:.1f}%\")\n",
    "            \n",
    "            # SenNet percentages  \n",
    "            sennet_supported_latest = df.loc['SenNet Supported Datasets', latest_col]\n",
    "            sennet_registered_latest = df.loc['SenNet Registered Datasets', latest_col]\n",
    "            if sennet_supported_latest > 0:\n",
    "                sennet_coverage = (sennet_registered_latest / sennet_supported_latest) * 100\n",
    "                print(f\"  SenNet: {sennet_registered_latest}/{sennet_supported_latest} = {sennet_coverage:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV as DataFrame: {e}\")\n",
    "        # Fallback to simple text display\n",
    "        with open(CSV_OUTPUT_PATH, 'r') as f:\n",
    "            print(f.read())\n",
    "else:\n",
    "    print(f\"CSV file not found at {CSV_OUTPUT_PATH}\")\n",
    "    print(\"Run the data collection cells above to generate the file.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
