{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd51504",
   "metadata": {},
   "source": [
    "## List of Supported Reference Organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766dfe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dequeue/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Supported Reference Organs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# HRA API endpoint for reference organs\n",
    "REFERENCE_ORGANS_URL = \"https://apps.humanatlas.io/api/v1/reference-organs\"\n",
    "\n",
    "# Get all reference organs\n",
    "response = requests.get(REFERENCE_ORGANS_URL)\n",
    "organs = response.json()\n",
    "\n",
    "# Extract the uberon values for all reference organs\n",
    "uberon_ids = [organ[\"representation_of\"] for organ in organs]\n",
    "\n",
    "print(f\"{len(organs)} Supported Reference Organs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f72f7e",
   "metadata": {},
   "source": [
    "## Ratio of Registered/Total for HuBMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7949e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BL', 'BR', 'HT', 'LE', 'LI', 'LK', 'LV', 'PA', 'RE', 'RK', 'SI', 'SK', 'SP', 'TH', 'TR', 'UT']\n",
      "Total datasets: 9751\n",
      "Supported datasets: 6366\n",
      "Registered datasets: 4438\n",
      "Top 10 blocks by dataset count that lack rui_location:\n",
      "https://portal.hubmapconsortium.org/browse/sample/a4c30e6b188093a5ad0bea83ce7249c0: 49\n",
      "https://portal.hubmapconsortium.org/browse/sample/5f27b70a8f50ef38970905020ac24bc3: 49\n",
      "https://portal.hubmapconsortium.org/browse/sample/33cb9c1e2e3cac4eb39bacb688a5834c: 24\n",
      "https://portal.hubmapconsortium.org/browse/sample/b07c38f5497330900cd74effb1468aa5: 19\n",
      "https://portal.hubmapconsortium.org/browse/sample/a77d4bc5656ce4051efc6317f51cf715: 16\n",
      "https://portal.hubmapconsortium.org/browse/sample/abef90ab7a7becada62c5d7bdda8a025: 16\n",
      "https://portal.hubmapconsortium.org/browse/sample/c1ec99d9f5da2d64fd7d6d2991604ce8: 16\n",
      "https://portal.hubmapconsortium.org/browse/sample/0129caf0e66fe67754475dc71e6706de: 8\n",
      "https://portal.hubmapconsortium.org/browse/sample/ab298067e1f49f85f28444c1b46f4766: 5\n",
      "https://portal.hubmapconsortium.org/browse/sample/b8ab66c75e67ba8a655b30d2b6a2c17d: 4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "# --- User-provided HuBMAP API token ---\n",
    "HUBMAP_TOKEN = \"Ag18QgjdyzJ8g5K8DGjQXdQK798Pg8xgambx8lKzVE8zya3qnVf2C6okv43vgJEr4opelP9j5bVBxYt9g555qFMx0rN\"\n",
    "\n",
    "# --- Step 1: Extract code_to_uberon mapping from metadata.js ---\n",
    "metadata_path = \"/Users/dequeue/Desktop/RUI.nosync/hra-registrations/scripts/metadata.js\"\n",
    "code_to_uberon = {}\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "    matches = re.findall(r\"(\\w+):\\s*{\\s*code: '(\\w+)',\\s*label: '[^']+',\\s*organ_id: '([^']+)'\", content)\n",
    "    for code, _, uberon in matches:\n",
    "        code_to_uberon[code] = uberon\n",
    "\n",
    "# --- Step 2: Get all reference organs and their UBERON IDs ---\n",
    "REFERENCE_ORGANS_URL = \"https://apps.humanatlas.io/api/v1/reference-organs\"\n",
    "response = requests.get(REFERENCE_ORGANS_URL)\n",
    "organs = response.json()\n",
    "\n",
    "# Extract and normalize the UBERON IDs from the API\n",
    "def iri_to_curie(iri):\n",
    "    if iri and \"obo/UBERON_\" in iri:\n",
    "        return \"UBERON:\" + iri.split(\"_\")[-1]\n",
    "    return iri\n",
    "\n",
    "reference_uberon_ids = {iri_to_curie(organ.get(\"representation_of\")) for organ in organs if organ.get(\"representation_of\")}\n",
    "\n",
    "# --- Step 3: Filter supported codes to only those whose UBERON IDs are in the reference set ---\n",
    "filtered_supported_codes = [code for code, uberon in code_to_uberon.items() if uberon in reference_uberon_ids]\n",
    "\n",
    "print(filtered_supported_codes)\n",
    "\n",
    "SEARCH_API_URL = \"https://search.api.hubmapconsortium.org/v3/search\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUBMAP_TOKEN}\"}\n",
    "\n",
    "# 1. Total count of all datasets\n",
    "total_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"match\": {\"entity_type.keyword\": \"Dataset\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 0\n",
    "}\n",
    "\n",
    "# 2. Count of datasets where organ matches filtered_supported_codes\n",
    "organ_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 0\n",
    "}\n",
    "\n",
    "# Get total count\n",
    "total_datasets_hubmap = requests.post(SEARCH_API_URL, json=total_query, headers=headers)\n",
    "total_datasets_count_hubmap = total_datasets_hubmap.json()['hits']['total']['value']\n",
    "\n",
    "# Get organ-matched count\n",
    "supported_datasets_hubmap = requests.post(SEARCH_API_URL, json=organ_query, headers=headers)\n",
    "supported_datasets_count_hubmap = supported_datasets_hubmap.json()['hits']['total']['value']\n",
    "\n",
    "print(f\"Total datasets: {total_datasets_count_hubmap}\")\n",
    "print(f\"Supported datasets: {supported_datasets_count_hubmap}\")\n",
    "\n",
    "# 3. Count of datasets with organ in filtered_supported_codes AND rui_location present in any ancestor\n",
    "organ_with_rui_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}},\n",
    "                {\"exists\": {\"field\": \"ancestors.rui_location\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"size\": 0\n",
    "}\n",
    "\n",
    "# Get count of datasets with rui_location in ancestors\n",
    "datasets_with_rui_hubmap = requests.post(SEARCH_API_URL, json=organ_with_rui_query, headers=headers)\n",
    "datasets_with_rui_hubmap_count = datasets_with_rui_hubmap.json()['hits']['total']['value']\n",
    "\n",
    "print(f\"Registered datasets: {datasets_with_rui_hubmap_count}\")\n",
    "\n",
    "# 4. Find datasets with organ in filtered_supported_codes and NO rui_location in any ancestor\n",
    "# Query for datasets with organ in filtered_supported_codes and NO rui_location in any ancestor,\n",
    "from collections import Counter\n",
    "\n",
    "datasets_without_rui_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"match\": {\"entity_type.keyword\": \"Dataset\"}},\n",
    "                {\"terms\": {\"origin_samples.organ.keyword\": filtered_supported_codes}},\n",
    "                {\"match\": {\"origin_samples.sample_category.keyword\": \"organ\"}}\n",
    "            ],\n",
    "            \"must_not\": [\n",
    "                {\"exists\": {\"field\": \"ancestors.rui_location\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"ancestors\"],\n",
    "    \"size\": 10000  # Increase if needed to capture all relevant datasets\n",
    "}\n",
    "\n",
    "datasets_without_rui_hubmap = requests.post(SEARCH_API_URL, json=datasets_without_rui_query, headers=headers)\n",
    "datasets = datasets_without_rui_hubmap.json()['hits']['hits']\n",
    "\n",
    "\n",
    "# 5. Output the three main counts to CSV\n",
    "csv_path = \"/Users/dequeue/Desktop/RUI.nosync/hra-registrations/scripts/hubmap_counts.csv\"\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "# Prepare the new column data\n",
    "new_column = [total_datasets_count_hubmap, supported_datasets_count_hubmap, datasets_with_rui_hubmap_count]\n",
    "\n",
    "# Check if file exists\n",
    "file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "if not file_exists:\n",
    "    # Create file with row labels and first column\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"\", \"Total Datasets\", \"Supported Datasets\", \"Registered Datasets\"])\n",
    "        writer.writerow([now] + new_column)\n",
    "else:\n",
    "    # Read existing data\n",
    "    with open(csv_path, \"r\", newline=\"\") as f:\n",
    "        rows = list(csv.reader(f))\n",
    "    # If the first column is empty, fill it with row labels\n",
    "    if rows and rows[0][0] == \"\":\n",
    "        for i, label in enumerate([\"Total Datasets\", \"Supported Datasets\", \"Registered Datasets\"], start=1):\n",
    "            if len(rows) <= i:\n",
    "                rows.append([label])\n",
    "            else:\n",
    "                rows[i][0] = label\n",
    "    # Append new column to each row\n",
    "    if len(rows) < 4:\n",
    "        # Ensure there are 4 rows (header + 3 data)\n",
    "        while len(rows) < 4:\n",
    "            rows.append([\"\"])\n",
    "    rows[0].append(now)\n",
    "    for i, val in enumerate(new_column, start=1):\n",
    "        rows[i].append(val)\n",
    "    # Write back to file\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# 6. Find and print the top 10 unregistered blocks by dataset count\n",
    "\n",
    "# Query all samples and get their uuid and sample_category\n",
    "samples_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"match\": {\"entity_type.keyword\": \"Sample\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"uuid\", \"sample_category\"],\n",
    "    \"size\": 10000  # Increase if needed\n",
    "}\n",
    "\n",
    "samples_resp = requests.post(SEARCH_API_URL, json=samples_query, headers=headers)\n",
    "sample_hits = samples_resp.json()['hits']['hits']\n",
    "\n",
    "# Build the mapping\n",
    "sample_category_map = {}\n",
    "for hit in sample_hits:\n",
    "    src = hit['_source']\n",
    "    uuid = src.get('uuid')\n",
    "    cat = src.get('sample_category', '')\n",
    "    if uuid:\n",
    "        sample_category_map[uuid] = cat.lower()\n",
    "\n",
    "\n",
    "ancestor_id_counter = Counter()\n",
    "\n",
    "for ds in datasets:\n",
    "    for ancestor in ds['_source'].get('ancestors', []):\n",
    "        uuid = ancestor.get('uuid')\n",
    "        if uuid and sample_category_map.get(uuid) == 'block':\n",
    "            ancestor_id_counter[uuid] += 1\n",
    "\n",
    "print(\"Top 10 blocks by dataset count that lack rui_location:\")\n",
    "for uuid, count in ancestor_id_counter.most_common(10):\n",
    "     print(f\"https://portal.hubmapconsortium.org/browse/sample/{uuid}: {count}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
